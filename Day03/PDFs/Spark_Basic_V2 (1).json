{"paragraphs":[{"title":"1. Map Function","text":"val rdd = sc.parallelize(0 until 100)\nval res = rdd.map(x => x*2)\nres.collect()","dateUpdated":"2018-12-05T11:16:24-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002548_-1532493057","id":"20170523-115209_150170221","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:304","user":"train307","dateFinished":"2018-12-05T11:16:26-0600","dateStarted":"2018-12-05T11:16:24-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:29\n\nres18: Array[Int] = Array(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198)\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize(range(0 , 100))\nres = rdd.map(lambda x : x*2)\nres.collect()","dateUpdated":"2018-12-05T11:17:26-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002549_-1532877806","id":"20181204-144152_1151606512","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:305","user":"train307","dateFinished":"2018-12-05T11:17:30-0600","dateStarted":"2018-12-05T11:17:26-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198]\n"}]}},{"title":"2. reduce Function","text":"val rdd = sc.parallelize(1 until 10)\nval res = rdd.reduce(_ + _)","dateUpdated":"2018-12-05T11:18:41-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002549_-1532877806","id":"20170523-115411_1338753673","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:306","user":"train307","dateFinished":"2018-12-05T11:18:42-0600","dateStarted":"2018-12-05T11:18:41-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:27\n\nres: Int = 45\n"}]}},{"text":"%pyspark \nrdd = sc.parallelize(range(0,10))\nres = rdd.reduce( lambda x, y: x +y )\nres","dateUpdated":"2018-12-05T11:21:15-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002549_-1532877806","id":"20181204-144211_1460501157","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307","user":"train307","dateFinished":"2018-12-05T11:21:15-0600","dateStarted":"2018-12-05T11:21:15-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"45\n"}]}},{"title":"3. flatmap function","text":"val rdd = sc.parallelize(List(List(1,2,3,4), List(5,6,7,8)))\nval res = rdd.flatMap(x => x)\nres.collect()\n\nval res = rdd.flatMap(x => x.map(y => y*2))\nres.collect()","dateUpdated":"2018-12-05T11:38:42-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002549_-1532877806","id":"20170523-115259_1368017428","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:308","user":"train307","dateFinished":"2018-12-05T11:38:43-0600","dateStarted":"2018-12-05T11:38:42-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[List[Int]] = ParallelCollectionRDD[7] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[8] at flatMap at <console>:29\n\nres19: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8)\n\nres: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at flatMap at <console>:29\n\nres20: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16)\n"}]}},{"text":"%pyspark\n\nrdd = sc.parallelize( [ [1,2,3,4], [5,6,7,8]])\nres = rdd.flatMap(lambda x: x)\nprint res.collect()\nres =rdd.flatMap (lambda x : [i * 2 for i in x])\nprint res.collect()","dateUpdated":"2018-12-05T11:38:50-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002549_-1532877806","id":"20181204-143253_701896588","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:309","user":"train307","dateFinished":"2018-12-05T11:38:50-0600","dateStarted":"2018-12-05T11:38:50-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 2, 3, 4, 5, 6, 7, 8]\n[2, 4, 6, 8, 10, 12, 14, 16]\n"}]}},{"title":"4. mapPartitions function","text":"val rdd = sc.parallelize(0 until 8, 3)\nrdd.collect\nval res = rdd.mapPartitions(x => List(x.sum).iterator)\nres.collect()","dateUpdated":"2018-12-05T11:54:06-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002550_-1531723559","id":"20170523-115337_1556285525","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:310","user":"train307","dateFinished":"2018-12-05T11:54:06-0600","dateStarted":"2018-12-05T11:54:06-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[25] at parallelize at <console>:27\n\nres26: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7)\n\nres: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[26] at mapPartitions at <console>:29\n\nres27: Array[Int] = Array(1, 9, 18)\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize(range(0, 8), 3)\nres = rdd.mapPartitions (lambda x: [sum(x)])\nprint res.collect()","dateUpdated":"2018-12-05T11:54:08-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002550_-1531723559","id":"20181204-151101_1948537067","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:311","user":"train307","dateFinished":"2018-12-05T11:54:08-0600","dateStarted":"2018-12-05T11:54:08-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 5, 22]\n"}]}},{"title":"5. filter function","text":"val rdd = sc.parallelize(0 until 100)\nval res = rdd.filter(x => x%2 == 0)\nres.collect()","dateUpdated":"2018-12-05T11:45:25-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002550_-1531723559","id":"20170523-115327_1071773324","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312","user":"train307","dateFinished":"2018-12-05T11:45:25-0600","dateStarted":"2018-12-05T11:45:25-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at filter at <console>:29\n\nres23: Array[Int] = Array(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98)\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize(range(0, 100))\nres = rdd.filter(lambda x: x%2 ==0)\nres.collect()","dateUpdated":"2018-12-05T11:45:41-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002550_-1531723559","id":"20181204-174633_737830830","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:313","user":"train307","dateFinished":"2018-12-05T11:45:41-0600","dateStarted":"2018-12-05T11:45:41-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]\n"}]}},{"title":"6. groupBy Function","text":"val rdd = sc.parallelize(0 until 100)\nval res = rdd.groupBy(x => x%2 == 0)\nres.collect.foreach(println)","dateUpdated":"2018-12-05T11:56:53-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002550_-1531723559","id":"20170523-115402_1545385034","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314","user":"train307","dateFinished":"2018-12-05T11:56:53-0600","dateStarted":"2018-12-05T11:56:53-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[(Boolean, Iterable[Int])] = ShuffledRDD[31] at groupBy at <console>:29\n(true,CompactBuffer(0, 2, 50, 52, 12, 14, 62, 64, 30, 32, 80, 82, 46, 48, 96, 98, 16, 18, 66, 68, 42, 44, 92, 94, 20, 22, 24, 70, 72, 74, 4, 6, 54, 56, 38, 40, 88, 90, 8, 10, 58, 60, 26, 28, 76, 78, 34, 36, 84, 86))\n(false,CompactBuffer(41, 43, 91, 93, 29, 31, 79, 81, 1, 3, 51, 53, 5, 7, 55, 57, 45, 47, 49, 95, 97, 99, 13, 15, 63, 65, 17, 19, 67, 69, 25, 27, 75, 77, 9, 11, 33, 35, 59, 61, 83, 85, 21, 23, 71, 73, 37, 39, 87, 89))\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize(range(0, 100))\nres = rdd.groupBy(lambda x: x%2 ==0)\nres2=res.map(lambda x, y : x.join(' '.join(map(str, y ))))\nprint(\"(%s, %s) \" % (res.collect()[0][0], ' '.join(map(str, res.collect()[0][1])) ))\nprint(\"(%s, %s) \" % (res.collect()[1][0], ' '.join(map(str, res.collect()[1][1])) ))","dateUpdated":"2018-12-05T11:57:17-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002551_-1532108308","id":"20181204-175016_1029639810","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:315","user":"train307","dateFinished":"2018-12-05T11:57:17-0600","dateStarted":"2018-12-05T11:57:17-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(False, 5 7 29 31 77 79 53 55 21 23 69 71 13 15 61 63 9 11 57 59 41 43 89 91 37 39 85 87 33 35 81 83 1 3 49 51 25 27 73 75 45 47 93 95 97 99 17 19 65 67) \n(True, 44 46 16 18 64 66 92 94 96 98 0 2 48 50 12 14 60 62 24 26 72 74 40 42 88 90 8 10 56 58 32 34 80 82 4 6 52 54 28 30 36 38 76 78 84 86 20 22 68 70) \n"}]}},{"title":"7. zipWithIndex Function","text":"val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\nval res = rdd.zipWithIndex()\nres.collect()","dateUpdated":"2018-12-05T11:59:00-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002551_-1532108308","id":"20170523-115343_1530976304","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:316","user":"train307","dateFinished":"2018-12-05T11:59:01-0600","dateStarted":"2018-12-05T11:59:00-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[38] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[39] at zipWithIndex at <console>:29\n\nres29: Array[(String, Long)] = Array((a,0), (b,1), (c,2), (d,3))\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize( [ \"a\", \"b\", \"c\", \"d\" ])\nres = rdd.zipWithIndex()\nres.collect()","dateUpdated":"2018-12-05T11:59:14-0600","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002551_-1532108308","id":"20181204-181853_1515639480","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:317","user":"train307","dateFinished":"2018-12-05T11:59:14-0600","dateStarted":"2018-12-05T11:59:14-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n"}]}},{"title":"8. groupByKey Function","text":"val rdd = sc.parallelize(List(\"a\", \"b\", \"a\", \"b\")).zipWithIndex\nval res = rdd.groupByKey()\nres.collect()","dateUpdated":"2018-12-05T11:59:28-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002551_-1532108308","id":"20170523-115429_578090431","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318","user":"train307","dateFinished":"2018-12-05T11:59:29-0600","dateStarted":"2018-12-05T11:59:28-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[44] at zipWithIndex at <console>:27\n\nres: org.apache.spark.rdd.RDD[(String, Iterable[Long])] = ShuffledRDD[45] at groupByKey at <console>:29\n\nres30: Array[(String, Iterable[Long])] = Array((a,CompactBuffer(0, 2)), (b,CompactBuffer(1, 3)))\n"}]}},{"text":"%pyspark\nrdd = sc.parallelize( [ \"a\", \"b\", \"a\", \"b\" ]).zipWithIndex()\nres = rdd.groupByKey()\nres.collect()","dateUpdated":"2018-12-05T11:59:31-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002551_-1532108308","id":"20181204-182058_21014984","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:319","user":"train307","dateFinished":"2018-12-05T11:59:32-0600","dateStarted":"2018-12-05T11:59:31-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[('a', <pyspark.resultiterable.ResultIterable object at 0x2aff78b069d0>), ('b', <pyspark.resultiterable.ResultIterable object at 0x2aff78b06110>)]\n"}]}},{"title":"9. ReduceByKey Function","text":"val rdd = sc.parallelize( List(\"a\", \"b\", \"a\", \"b\")).zipWithIndex()\nval res = rdd.reduceByKey(_ + _)\nres.collect()","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20170523-115432_2027652641","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"text":"%pyspark\nrdd = sc.parallelize( [ \"a\", \"b\", \"a\", \"b\" ]).zipWithIndex()\nres = rdd.reduceByKey(lambda x, y: x +y)\nres.collect()","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20181204-183020_181239690","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:321"},{"title":"10. Join function ","text":"val person = List(\"adam\", \"ben\", \"chris\", \"david\")\nval age = List(27, 42, 53, 23)\nval dept = List(\"HPC\", \"Data\", \"Vis\", \"Edu\")\nval rdd1 = sc.parallelize(person.zip(age))\nval rdd2 = sc.parallelize(person.zip(dept))\nval res = rdd1.join(rdd2)\nres.collect()","dateUpdated":"2018-12-05T12:05:50-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20170523-115448_1474428706","dateCreated":"2018-12-05T11:13:22-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:322","user":"train307","dateFinished":"2018-12-05T12:05:51-0600","dateStarted":"2018-12-05T12:05:50-0600","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nperson: List[String] = List(adam, ben, chris, david)\n\nage: List[Int] = List(27, 42, 53, 23)\n\ndept: List[String] = List(HPC, Data, Vis, Edu)\n\nrdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[53] at parallelize at <console>:31\n\nrdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[54] at parallelize at <console>:31\n\nres: org.apache.spark.rdd.RDD[(String, (Int, String))] = MapPartitionsRDD[57] at join at <console>:37\n\nres31: Array[(String, (Int, String))] = Array((ben,(42,Data)), (david,(23,Edu)), (adam,(27,HPC)), (chris,(53,Vis)))\n"}]}},{"text":"%pyspark \nperson = sc.parallelize([\"adam\", \"ben\", \"chris\", \"david\"])\nage = sc.parallelize([27, 42, 53, 23])\ndept = sc.parallelize([\"HPC\", \"Data\", \"Vis\", \"Edu\"])\nrdd1 = person.zip(age)\nrdd2 = person.zip(dept)\nres = rdd1.join(rdd2)\nres.collect()","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20181204-194908_1327981886","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"title":"11. wordcount","text":"val lines = sc.textFile(\"/tmp/data/book.txt\")\nval words = lines.flatMap(l => l.split(\" \"))\n                 .map(w => (w, 1))\n                 .reduceByKey(_ + _)\n                 .sortBy(_._2, false).collect\n","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20170523-120016_483577617","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%pyspark\nlines = sc.textFile(\"SPARK.out\")\nwords = lines.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b) \\\n             .sortBy(lambda a: a[1], False)\nwords.collect()","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002552_-1534032053","id":"20181204-183130_229936641","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"title":"12. Calculating Pi","text":"val NUM_SAMPLES=100000\nval count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>\n  val x = math.random\n  val y = math.random\n  x*x + y*y < 1\n}.count()\nprintln(s\"Pi is roughly ${4.0 * count / NUM_SAMPLES}\")","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002553_-1534416802","id":"20181204-195401_201986694","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"%pyspark\nimport random as random\nNUM_SAMPLES=100000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\ncount = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n             .filter(inside).count()\nprint \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002553_-1534416802","id":"20181204-201431_1798209713","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"title":"13 Dataframe Basics","text":"//loading file from local file system\n/*val df=spark.read.format(\"csv\")\n            .option(\"header\", true)\n            .load(\"/tmp/data/mtcars.csv\")\n*/\n//loading file from hdfs file system \nval df=spark.read.format(\"csv\")\n            .option(\"header\", true)\n            .load(\"/tmp/data/mtcars.csv\")            \n\ndf.show()\ndf.printSchema\ndf.describe().show\ndf.describe(\"mpg\").show\n","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002553_-1534416802","id":"20181205-053320_1305284416","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"title":"14. Read and Write with different file format","text":"df.show(100)\ndf.write.format(\"json\").mode(\"overwrite\").save(\"cars.json\")\ndf.write.parquet(\"cars.parquet\")\ndf.write.option(\"delimiter\",\"\\t\").csv(\"cars.tab\")\nval df_json = spark.read.json(\"cars.json\")\nval df_parquet = spark.read.parquet(\"cars.parquet\")\ndf_json.show\ndf_parquet.show","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002553_-1534416802","id":"20181205-053635_1858817951","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"title":"15. RDD vs Dataframe vs. DataSet","text":"val rdd = sc.parallelize(0 until 100)\nval rdd_df =rdd.toDF\nval rdd_ds = rdd.toDS\n\nrdd.filter (_ < 10).collect\nrdd_df.filter(\"value < 10\").show\nrdd_ds.filter(\"value < 10\").show\nrdd_ds.filter(_ < 10).show\n\nrdd.map(_ * 2).collect\nrdd_ds.map(_ * 2).show\nrdd_df.map(_ * 2).show //this line will fail \nrdd_df.select('value * 2).show \n","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002553_-1534416802","id":"20181205-054915_1611902029","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"title":"16. RDD to Dataframe","text":"val rdd = sc.parallelize(0 until 10000)\nval simple_rdd_df = rdd.toDF\nsimple_rdd_df.show\n\ncase class Person(id: Int, name: String)\nval person = sc.parallelize(Seq( Person(1, \"Mike\"), \n                                 Person(2, \"Smith\"), \n                                 Person(3, \"Brooke\")))\nval person_df = person.toDF\nperson_df.show ","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002554_-1533262555","id":"20181205-063857_1111149783","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"title":"17. Spark SQL functions","text":"df.select(\"model\",\"mpg\", \"wt\").show\ndf.select($\"model\", $\"mpg\" * 1.6).show\ndf.select(\"model\",\"mpg\", \"wt\", \"cyl\")\n  .filter(\"cyl > 4\").show\ndf.groupBy(\"cyl\").count().show\n","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002554_-1533262555","id":"20181205-064203_1671027480","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"title":"18. Spark SQL statement","text":"df.createOrReplaceTempView(\"cars\")\nspark.sql(\"SELECT * FROM cars WHERE cyl = 6\")\n     .show\nspark.sql(\"SELECT cyl, avg(mpg) as avg_mpg, count(1) as count \"+\n          \"from cars \"+\n          \"group by cyl\")\n     .show","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002554_-1533262555","id":"20181205-064152_2090623997","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"title":"19. Analysis example","text":"%spark\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\n\nval cars = spark.read.format(\"csv\").option(\"header\", true).load(\"/tmp/data/mtcars.csv\")\n                .selectExpr(\"model\", \"mpg + 0.0 as mpg\", \"disp + 0.0 as disp\", \n                            \"hp + 0.0 as hp\", \"drat + 0.0 as drat\", \"wt + 0.0 as wt\", \n                            \"cyl + 0.0 as label\")\n                \nval training= cars.sample(false, 0.8)\nval test= cars.except(training)\n\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\"))\n  .setOutputCol(\"features\")\n  \nval lr = new LogisticRegression()\n  .setMaxIter(10)\n  .setRegParam(0.2)\n  .setElasticNetParam(0.0)\n\nval pipeline = new Pipeline().setStages(Array(assembler, lr))\nval lrModel = pipeline.fit(training)\n\nval result = lrModel.transform(test).select('model, 'label, 'prediction)\nresult.show","dateUpdated":"2018-12-05T11:13:22-0600","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544030002554_-1533262555","id":"20181205-064435_1022233507","dateCreated":"2018-12-05T11:13:22-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:334"}],"name":"Spark_Basic_V2","id":"2DY4XTCKD","angularObjects":{"2DZ7SDVMW:shared_process":[],"2DY75R1KE:shared_process":[],"2DXFUYS2M:shared_process":[],"2DZK5FGWQ:shared_process":[],"2DXZXTSY5:shared_process":[],"2DWXXJRRG:shared_process":[],"2DY69VWX6:shared_process":[],"2DW428PKE:shared_process":[],"2DZDV17XV:shared_process":[],"2DX3AC7FT:shared_process":[],"2DWV4MRWG:shared_process":[],"2DZMPPDED:shared_process":[],"2DZTUNC3F:shared_process":[],"2DYGS8Y2T:shared_process":[],"2DWZC1VU6:shared_process":[],"2DWAJHR1Y:shared_process":[],"2DY35149K:shared_process":[],"2DWXVHFR9:shared_process":[],"2DW17BW3Y:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}